{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "nltk.download('wordnet')  \n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Main Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv', sep=',')\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data\n",
    "\n",
    "1- Checking missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Checking distribution of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- LowerCase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- Tokenizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.heatmap(dataset.isnull().transpose(),cmap=\"RdBu_r\",cbar_kws={'label': 'Missing Data'})\n",
    "#No missing values visible, Good data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_num = dataset['class'].value_counts()\n",
    "\n",
    "# Define a custom color palette\n",
    "custom_palette = ['gold', 'lightcoral', 'lightskyblue', 'lightgreen']\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=dataset, x='class', palette=custom_palette)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming text into lowercase text\n",
    "dataset['text'] = dataset['text'].str.lower()\n",
    "dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctuation\n",
    "dataset['text'] = dataset['text'].str.replace(r'[^\\w\\s]+', '',regex=True)\n",
    "dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    clean_text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "    return clean_text\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text'] = dataset['text'].map(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_sentence = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text'] = dataset['text'].map(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('Clean Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Clean Dataset.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[dataset['text'].isnull()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[dataset['text'].isnull()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['text']\n",
    "y = dataset['class'].replace({'suicide':1,'non-suicide':0})\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=4)\n",
    "len(X_train),len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency Inverse Document Frequency ( TF-IDF Vectorizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_values = [5000, 10000]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    " \n",
    "param_grid = {\n",
    "    'max_features': max_features_values,\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(tfidf_vectorizer, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_max_features = grid_search.best_params_['max_features']\n",
    "\n",
    "print(f\"Best max_features: {best_max_features}\")\n",
    "\n",
    "best_tfidf_vectorizer = grid_search.best_estimator_\n",
    "\n",
    "pickle.dump(best_tfidf_vectorizer, open('grid_search_15000_tfidf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tfidf_vectorizer = pickle.load(open('grid_search_15000_tfidf.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = best_tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf = best_tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "num_train_samples, _ = X_train_tfidf.shape\n",
    "num_test_samples, _ = X_test_tfidf.shape\n",
    "\n",
    "print(f\"Number of training samples: {num_train_samples}\")\n",
    "print(f\"Number of test samples: {num_test_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your sparse TF-IDF matrices to dense numpy arrays\n",
    "X_train_dense = X_train_tfidf.toarray()\n",
    "X_test_dense = X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb2 = BernoulliNB()\n",
    "nb3 = MultinomialNB()\n",
    "VotingClassifiers = VotingClassifier(estimators=[('GaussianNB', nb),('BernoulliNB',nb2), ('MultinomialNB', nb3)], voting='soft')\n",
    "\n",
    "VotingClassifiers.fit(X_train_dense, y_train)\n",
    "\n",
    "pickle.dump(VotingClassifiers, open('Voting_classifier.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VotingClassifiers = pickle.load(open('Voting_classifier.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training score:', VotingClassifiers.score(X_train_dense, y_train))\n",
    "print('Testing score:', VotingClassifiers.score(X_test_dense, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model on a New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = pd.read_csv('test_suicide.csv')\n",
    "new_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset['Tweet'] = new_dataset['Tweet'].str.lower()\n",
    "new_dataset['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset['Tweet'] = new_dataset['Tweet'].str.replace(r'[^\\w\\s]|(\\d+\\.\\d+|\\d+)', '', regex=True)\n",
    "new_dataset['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove__stop_words(text):\n",
    "    clean_text = ' '.join([word for word in str(text).split() if word.lower() not in stop_words])\n",
    "    return clean_text\n",
    " \n",
    "new_dataset['Tweet'] = new_dataset['Tweet'].apply(remove__stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset['Tweet'] = new_dataset['Tweet'].map(tokenize_text)\n",
    "new_dataset['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset['Tweet'] = new_dataset['Tweet'].map(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_dataset[new_dataset['Tweet'].isnull()].index) #No missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = new_dataset['Tweet']\n",
    "y_new = new_dataset['Suicide'].replace({'Potential Suicide post ':1,'Not Suicide post':0})\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new.iloc[500:510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tfidf_vectorizer = pickle.load(open('grid_search_15000_tfidf.pkl', 'rb'))\n",
    "VotingClassifiers = pickle.load(open('Voting_classifier.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_tfidf = best_tfidf_vectorizer.transform(X_new)\n",
    "X_new_tfidf = X_new_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing score:', VotingClassifiers.score(X_new_tfidf, y_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_dense = sc.fit_transform(X_train_dense)\n",
    "X_test_dense = sc.transform(X_test_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_KNN = KNeighborsClassifier(n_neighbors = 10, metric = 'minkowski', p = 2)\n",
    "classifier_KNN.fit(X_train_dense, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training score:', classifier_KNN.score(X_train_dense, y_train))\n",
    "print('Testing score:', classifier_KNN.score(X_test_dense, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're keeping the Naives Bayes because they have the best accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Function for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(element):\n",
    "    element = element.lower() #convert to lower case \n",
    "    element = element.replace(r'[^\\w\\s]+', '') #remove punctuations\n",
    "    element = [word for word in element.split() if word not in (stop_words)] #tokenize the sentence\n",
    "    element = ' '.join([lemmatizer.lemmatize(i) for i in element]) #lemmatizing\n",
    "    inputToModel = best_tfidf_vectorizer.transform([element]).toarray() #transform to vector form\n",
    "    return inputToModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to predict preprocessed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_voting(input_text):\n",
    "    print('Input : ',input_text) \n",
    "    processed_array = preprocess(input_text) \n",
    "    predict = VotingClassifiers.predict(processed_array) #Model prediction\n",
    "    if predict[0] == 1:\n",
    "        print('Output : Suicidal thought detected')\n",
    "    else:\n",
    "        print('Output : No suicidal thoughts detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_voting(\"My life is so miserable and it's not getting better.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_voting(\"Let's go for a hike !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix For accuracy Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = VotingClassifiers.predict(X_test_dense)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.set(font_scale=1.4)  \n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['No Suicide', 'Suicide'],\n",
    "            yticklabels=['No Suicide', 'Suicide'])\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The calibration curve assesses how well a binary classification model's\n",
    "#predicted probabilities match actual outcomes. It plots the average predicted probability\n",
    "#against the actual fraction of positive cases. A well-calibrated model's curve aligns closely with the ideal diagonal line.\n",
    "\n",
    "y_prob = VotingClassifiers.predict_proba(X_test_dense)[:, 1]\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10, strategy='uniform')\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='-', color='b', label='Calibration Curve')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = VotingClassifiers.predict(X_test_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "print(\"Shape of y_pred:\", y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.DataFrame(np.array(y_test).reshape(1,-1))\n",
    "y_pred = pd.DataFrame(np.array(y_pred).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "print(\"Shape of y_pred:\", y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='micro')\n",
    "recall = recall_score(y_test, y_pred, average='micro')\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Precision', 'Recall', 'F1-Score'],\n",
    "    'Score': [precision, recall, f1]\n",
    "})\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "g = sns.catplot(x='Metric', y='Score', data=metrics_df, kind='bar', palette='Set2', height=5, aspect=2)\n",
    "\n",
    "g.despine(left=True)\n",
    "g.set_ylabels('Score')\n",
    "g.set(ylim=(0, 1.0)) \n",
    "plt.title('Classification Metrics')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67c68f31e0f161d33e3c839aa4ab87532465ceee3ae3a88b4bf93127eda9841d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('myenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
